{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the csv file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Finds the spark path \n",
    "import findspark\n",
    "findspark.init()\n",
    "\n",
    "import pyspark\n",
    "import pyspark.sql\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.ml.feature import StringIndexer\n",
    "from pyspark.ml import Pipeline\n",
    "\n",
    "spark = SparkSession.builder \\\n",
    "     .master(\"local\") \\\n",
    "     .appName(\"hotels\") \\\n",
    "     .getOrCreate()\n",
    "\n",
    "hotels_df = spark.read.csv(\"../input/Hotels_data_Changed.csv\", header=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get the highest discount code for features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import Row\n",
    "\n",
    "def rowToKeyValue(row):\n",
    "    key = (row['WeekDay'], row[\"Snapshot Date\"], row[\"Checkin Date\"], row[\"DayDiff\"], row[\"Hotel Name\"])\n",
    "    val = ([row[\"Discount Code\"]], row['DiscountPerc'])\n",
    "    return (key,val)\n",
    "\n",
    "def reduceToMaxDiscountPerKey(val1, val2):\n",
    "    codes1, discount1 = val1\n",
    "    codes2, discount2 = val2\n",
    "    if (discount1 > discount2):\n",
    "        return val1\n",
    "    elif(discount2 > discount1):\n",
    "        return val2\n",
    "    else: # In case the discounts are equals, merge the prices to same array\n",
    "        return (codes1+ codes2, discount1)\n",
    "\n",
    "def flatMapDiscountCodes(row):\n",
    "    key, val = row\n",
    "    codes = val[0]\n",
    "    # Return list of key & code\n",
    "    return [(key, code) for code in codes]\n",
    "\n",
    "def rddToRow(rddRow):\n",
    "    return Row(WeekDay=rddRow[0][0], SnapshotDate=rddRow[0][1], CheckinDate=rddRow[0][2],\\\n",
    "                DayDiff=rddRow[0][3], HotelName=rddRow[0][4], DiscountCode=rddRow[1])\n",
    "\n",
    "hotelsBestDiscountCode_df = hotels_df.rdd\\\n",
    "                .map(rowToKeyValue)\\\n",
    "                .reduceByKey(reduceToMaxDiscountPerKey)\\\n",
    "                .flatMap(flatMapDiscountCodes)\\\n",
    "                .map(rddToRow).toDF()\n",
    "hotelsBestDiscountCode_df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Normalize the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col, udf\n",
    "from pyspark.sql.types import IntegerType\n",
    "import pandas as pd\n",
    "\n",
    "# Transform string values to numeric\n",
    "indexers = [StringIndexer(inputCol=\"WeekDay\", outputCol=\"WeekDayIndex\"),\n",
    "            StringIndexer(inputCol=\"HotelName\", outputCol=\"HotelNameIndex\"),]\n",
    "pipeline = Pipeline(stages=indexers)\n",
    "hotelsWithIndexedStrings_df = pipeline.fit(hotelsBestDiscountCode_df).transform(hotelsBestDiscountCode_df)\n",
    "\n",
    "# Extract date values\n",
    "dateYearValue = udf(lambda x: pd.to_datetime(x).year, IntegerType())\n",
    "dateDayValue = udf(lambda x: pd.to_datetime(x).day, IntegerType())\n",
    "dateMonthValue = udf(lambda x: pd.to_datetime(x).month, IntegerType())\n",
    "\n",
    "hotelsWithDateIndexed_df = hotelsWithIndexedStrings_df\\\n",
    "                     .withColumn('SnapshotDateYear', dateYearValue(col('SnapshotDate')))\\\n",
    "                     .withColumn('SnapshotDateMonth', dateMonthValue(col('SnapshotDate')))\\\n",
    "                     .withColumn('SnapshotDateDay', dateDayValue(col('SnapshotDate')))\\\n",
    "                     .withColumn('CheckinDateYear', dateYearValue(col('CheckinDate')))\\\n",
    "                     .withColumn('CheckinDateMonth', dateMonthValue(col('CheckinDate')))\\\n",
    "                     .withColumn('CheckinDateDay', dateDayValue(col('CheckinDate')))\n",
    "# Convert string column to int\n",
    "hotelsWithIntCoulmn_df = hotelsWithDateIndexed_df.withColumn(\"DayDiff\",\\\n",
    "                                   hotelsWithDateIndexed_df[\"DayDiff\"].cast(\"integer\"))\\\n",
    "                                .withColumn('DiscountCode', hotelsWithDateIndexed_df['DiscountCode'].cast('integer'))\n",
    "\n",
    "# Remove unneccesary columns\n",
    "hotelsWithoutColumns_df = hotelsWithIntCoulmn_df.drop('SnapshotDate').drop('CheckinDate')\\\n",
    "                         .drop('HotelName').drop('WeekDay')\n",
    "hotelsWithoutColumns_df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Distinct values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col, countDistinct\n",
    "\n",
    "def transposeDF(df):\n",
    "    pandas_df = df.toPandas().transpose().reset_index()\n",
    "    return spark.createDataFrame(pandas_df)\n",
    "    \n",
    "# Show distinct values count\n",
    "distinctValuesDF = hotelsWithoutColumns_df.agg(*(countDistinct(col(c)).alias(c) for c in hotelsWithoutColumns_df.columns));\n",
    "transposeDF(distinctValuesDF).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Columns statsitics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data exploration\n",
    "describe_df = hotelsWithoutColumns_df.describe(hotelsWithoutColumns_df.columns);\n",
    "transposeDF(describe_df).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Column correlations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for column in hotelsWithoutColumns_df.columns:\n",
    "    corr = hotelsWithoutColumns_df.corr('DiscountCode', column)\n",
    "    print(\"Column %s correlation: %s\" % (column, corr))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run decision tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.classification import DecisionTreeClassifier\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "\n",
    "# Create vector of all features expect the label\n",
    "assembler = VectorAssembler(\n",
    "    inputCols=[x for x in hotelsWithoutColumns_df.columns if x != 'DiscountCode'],\n",
    "    outputCol='features')\n",
    "\n",
    "# Split the data into training and test sets (30% held out for testing)\n",
    "(trainingData, testData) = hotelsWithoutColumns_df.randomSplit([0.7, 0.3])\n",
    "\n",
    "# Train a DecisionTree model.\n",
    "dt = DecisionTreeClassifier(labelCol='DiscountCode', featuresCol='features',\\\n",
    "                            impurity='entropy', maxDepth=20, maxBins=554)\n",
    "\n",
    "# Chain indexers and tree in a Pipeline\n",
    "pipeline = Pipeline(stages=[assembler, dt])\n",
    "\n",
    "# Train model.  This also runs the indexers.\n",
    "tree_model = pipeline.fit(trainingData)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "\n",
    "# Make predictions.\n",
    "predictions = tree_model.transform(testData)\n",
    "\n",
    "# Select example rows to display.\n",
    "predictions.select(\"prediction\", \"DiscountCode\", \"features\").show(5)\n",
    "\n",
    "# Select (prediction, true label) and compute test error\n",
    "evaluator = MulticlassClassificationEvaluator(\n",
    "    labelCol=\"DiscountCode\", predictionCol=\"prediction\", metricName=\"accuracy\")\n",
    "accuracy = evaluator.evaluate(predictions)\n",
    "print(\"Test Accuaracy = %g \" %  accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Print decision tree auc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.evaluation import BinaryClassificationEvaluator\n",
    "from pyspark.sql import functions as F\n",
    "\n",
    "# Binary classifications\n",
    "for label in range(1,5):\n",
    "    # Filter only relevanot to class predictions\n",
    "    binaryPredictions = predictions.where(\"prediction == %s OR DiscountCode == %s\" % (label, label))\\\n",
    "                .withColumn('binaryPrediction', F.when(col('prediction')==label,1.0).otherwise(0.0))\\\n",
    "                .withColumn('binaryDiscountCode', F.when(col('DiscountCode')==label,1.0).otherwise(0.0))\n",
    "            \n",
    "    binaryPredictions.select('prediction', 'DiscountCode', 'binaryPrediction', 'binaryDiscountCode').show(5)\n",
    "    \n",
    "    evaluator = BinaryClassificationEvaluator(rawPredictionCol=\"binaryPrediction\", labelCol=\"binaryDiscountCode\")\n",
    "    auc = evaluator.evaluate(binaryPredictions)\n",
    "    \n",
    "    print(\"Class %s area under roc = %s\" % (label, auc))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run naive bayes - old"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.mllib.classification import NaiveBayes, NaiveBayesModel\n",
    "\n",
    "model = NaiveBayes.train(training_data, 1.0)\n",
    "\n",
    "NaiveBayes_predictionAndLabel = test_data.map(lambda p: (float(model.predict(p.features)), p.label))\n",
    "\n",
    "naive_metrics = MulticlassMetrics(NaiveBayes_predictionAndLabel)\n",
    "\n",
    "print('Accuracy {}'.format(naive_metrics.accuracy))\n",
    "print('False positive rate {}'.format(naive_metrics.weightedFalsePositiveRate))\n",
    "print(naive_metrics.confusionMatrix())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run naive bayes - new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.classification import NaiveBayes\n",
    "\n",
    "# Split the data into training and test sets (30% held out for testing)\n",
    "(trainingData, testData) = hotelsWithoutColumns_df.randomSplit([0.9, 0.1])\n",
    "\n",
    "# Train a NaiveBayes model.\n",
    "dt = NaiveBayes(labelCol='DiscountCode', featuresCol='features')\n",
    "\n",
    "# Chain indexers and tree in a Pipeline\n",
    "pipeline = Pipeline(stages=[assembler, dt])\n",
    "\n",
    "# Train model.  This also runs the indexers.\n",
    "naive_model = pipeline.fit(trainingData)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Naive bayes model evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "\n",
    "# Make predictions.\n",
    "predictions = tree_model.transform(testData)\n",
    "\n",
    "# Select example rows to display.\n",
    "predictions.select(\"prediction\", \"DiscountCode\", \"features\").show(5)\n",
    "\n",
    "# Select (prediction, true label) and compute test error\n",
    "evaluator = MulticlassClassificationEvaluator(\n",
    "    labelCol=\"DiscountCode\", predictionCol=\"prediction\", metricName=\"accuracy\")\n",
    "accuracy = evaluator.evaluate(predictions)\n",
    "print(\"Test Accuaracy = %g \" %  accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Print naive bayes auc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.evaluation import BinaryClassificationEvaluator\n",
    "from pyspark.sql import functions as F\n",
    "\n",
    "# Binary classifications\n",
    "for label in range(1,5):\n",
    "    # Filter only relevanot to class predictions\n",
    "    binaryPredictions = predictions.where(\"prediction == %s OR DiscountCode == %s\" % (label, label))\\\n",
    "                .withColumn('binaryPrediction', F.when(col('prediction')==label,1.0).otherwise(0.0))\\\n",
    "                .withColumn('binaryDiscountCode', F.when(col('DiscountCode')==label,1.0).otherwise(0.0))\n",
    "            \n",
    "    binaryPredictions.select('prediction', 'DiscountCode', 'binaryPrediction', 'binaryDiscountCode').show(5)\n",
    "    \n",
    "    evaluator = BinaryClassificationEvaluator(rawPredictionCol=\"binaryPrediction\", labelCol=\"binaryDiscountCode\")\n",
    "    auc = evaluator.evaluate(binaryPredictions)\n",
    "    \n",
    "    print(\"Class %s area under roc = %s\" % (label, auc))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
