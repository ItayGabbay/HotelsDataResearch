{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the csv file and convert features to numeric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Finds the spark path \n",
    "import findspark\n",
    "findspark.init()\n",
    "\n",
    "import pyspark\n",
    "import pyspark.sql\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.ml.feature import StringIndexer\n",
    "from pyspark.ml import Pipeline\n",
    "\n",
    "spark = SparkSession.builder \\\n",
    "     .master(\"local\") \\\n",
    "     .appName(\"hotels\") \\\n",
    "     .getOrCreate()\n",
    "\n",
    "df = spark.read.csv(\"../input/Hotels_data_Changed.csv\", header=True)\n",
    "# Transform string values to numeric\n",
    "indexers = [StringIndexer(inputCol=\"WeekDay\", outputCol=\"WeekDayIndex\"),\n",
    "            StringIndexer(inputCol=\"Hotel Name\", outputCol=\"HotelNameIndex\"),]\n",
    "pipeline = Pipeline(stages=indexers)\n",
    "indexed_df = pipeline.fit(df).transform(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Row(_c0='0', Snapshot ID='1', Snapshot Date='2015-07-17', Checkin Date='2015-08-12', Days='5', Original Price='1178', Discount Price='1040', Discount Code='1', Available Rooms='6', Hotel Name='Best Western Plus Seaport Inn Downtown', Hotel Stars='3', DayDiff='26', WeekDay='Wed', DiscountDiff='138', DiscountPerc='11.714770797962649', WeekDayIndex=0.0, HotelNameIndex=153.0, SnapshotDateIndex=735796, CheckinDateIndex=735822)\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import col, udf\n",
    "from pyspark.sql.types import DoubleType, IntegerType\n",
    "import pandas as pd\n",
    "\n",
    "#Transform date values to numeric\n",
    "dateFormatter = udf(lambda x:  pd.to_datetime(x).toordinal(), IntegerType())\n",
    "\n",
    "indexed_df = indexed_df.withColumn('SnapshotDateIndex', dateFormatter(col('Snapshot Date')))\n",
    "indexed_df = indexed_df.withColumn('CheckinDateIndex', dateFormatter(col('Checkin Date')))\n",
    "\n",
    "print (indexed_df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get the highest discount code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rowToKeyValue(row):\n",
    "    key = (row['WeekDayIndex'], row[\"SnapshotDateIndex\"], row[\"CheckinDateIndex\"], float(row[\"DayDiff\"]), row[\"HotelNameIndex\"])\n",
    "    val = ([row[\"Discount Code\"]], row['DiscountPerc'])\n",
    "    return (key,val)\n",
    "\n",
    "def reduceToMaxDiscountPerKey(val1, val2):\n",
    "    codes1, discount1 = val1\n",
    "    codes2, discount2 = val2\n",
    "    if (discount1 > discount2):\n",
    "        return val1\n",
    "    elif(discount2 > discount1):\n",
    "        return val2\n",
    "    else: # In case the discounts are equals, merge the prices to same array\n",
    "        return (codes1+ codes2, discount1)\n",
    "\n",
    "def flatMapDiscountCodes(row):\n",
    "    key, val = row\n",
    "    codes = val[0]\n",
    "    # Return list of key & code\n",
    "    return [(key, code) for code in codes]\n",
    "    \n",
    "rdd = indexed_df.rdd.map(rowToKeyValue)\\\n",
    "    .reduceByKey(reduceToMaxDiscountPerKey)\\\n",
    "    .flatMap(flatMapDiscountCodes)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create test & training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.mllib.tree import DecisionTree, DecisionTreeModel\n",
    "from pyspark.mllib.regression import LabeledPoint\n",
    "\n",
    "def mapToLabeledPoint(tup):\n",
    "    key, val = tup\n",
    "    # Change range of values from 1-4 to 0-3\n",
    "    return LabeledPoint(int(val) -1, list(key))\n",
    "\n",
    "# Split into test and train data\n",
    "test_data, training_data = rdd.map(mapToLabeledPoint).randomSplit(weights=[0.3, 0.7], seed=1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run decision tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "features 0: Week Day\n",
      "features 1: Snapshot Date\n",
      "features 2: Checkin Date\n",
      "features 3: Day Diff\n",
      "features 4: Hotel Name\n"
     ]
    }
   ],
   "source": [
    "tree_model = DecisionTree.trainClassifier(training_data, numClasses=4, \n",
    "                                          categoricalFeaturesInfo={},\n",
    "                                          impurity='gini', maxDepth=20, maxBins=200)\n",
    "\n",
    "# Print results\n",
    "print('features 0: Week Day')\n",
    "print('features 1: Snapshot Date')\n",
    "print('features 2: Checkin Date')\n",
    "print('features 3: Day Diff')\n",
    "print('features 4: Hotel Name')\n",
    "# print(tree_model.toDebugString())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Print decision tree statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy 0.6326970620364515\n",
      "False positive rate 0.1335184927564076\n",
      "DenseMatrix([[ 4813.,  1375.,   876.,   670.],\n",
      "             [ 1342.,  7542.,  1572.,   702.],\n",
      "             [  910.,  1957.,  6118.,   789.],\n",
      "             [  744.,   952.,   888.,  3536.]])\n"
     ]
    }
   ],
   "source": [
    "from pyspark.mllib.evaluation import MulticlassMetrics\n",
    "\n",
    "predictions = tree_model.predict(test_data.map(lambda p: p.features))\n",
    "predictionAndLabels = predictions.zip(test_data.map(lambda p: p.label))\n",
    "\n",
    "metrics = MulticlassMetrics(predictionAndLabels)\n",
    "\n",
    "print('Accuracy {}'.format(metrics.accuracy))\n",
    "print('False positive rate {}'.format(metrics.weightedFalsePositiveRate))\n",
    "print(metrics.confusionMatrix())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run naive bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy 0.25142298625883974\n",
      "False positive rate 0.21618546897854687\n",
      "DenseMatrix([[  130.,  2424.,   230.,  4950.],\n",
      "             [  148.,  3547.,   649.,  6814.],\n",
      "             [  136.,  2872.,   656.,  6110.],\n",
      "             [   44.,  1426.,   237.,  4413.]])\n"
     ]
    }
   ],
   "source": [
    "from pyspark.mllib.classification import NaiveBayes, NaiveBayesModel\n",
    "\n",
    "# Train a naive Bayes model.\n",
    "model = NaiveBayes.train(training_data, 1.0)\n",
    "\n",
    "# Make prediction\n",
    "NaiveBayes_predictionAndLabel = test_data.map(lambda p: (float(model.predict(p.features)), p.label))\n",
    "\n",
    "naive_metrics = MulticlassMetrics(NaiveBayes_predictionAndLabel)\n",
    "\n",
    "print('Accuracy {}'.format(naive_metrics.accuracy))\n",
    "print('False positive rate {}'.format(naive_metrics.weightedFalsePositiveRate))\n",
    "print(naive_metrics.confusionMatrix())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
