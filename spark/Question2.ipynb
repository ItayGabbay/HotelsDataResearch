{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the csv file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Finds the spark path \n",
    "import findspark\n",
    "findspark.init()\n",
    "\n",
    "import pyspark\n",
    "import pyspark.sql\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.ml.feature import StringIndexer\n",
    "from pyspark.ml import Pipeline\n",
    "\n",
    "spark = SparkSession.builder \\\n",
    "     .master(\"local\") \\\n",
    "     .appName(\"hotels\") \\\n",
    "     .getOrCreate()\n",
    "\n",
    "hotels_df = spark.read.csv(\"../input/Hotels_data_Changed.csv\", header=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get the highest discount code for features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+-------+------------+--------------------+------------+-------+\n",
      "|CheckinDate|DayDiff|DiscountCode|           HotelName|SnapshotDate|WeekDay|\n",
      "+-----------+-------+------------+--------------------+------------+-------+\n",
      "| 2015-08-12|     26|           2|Best Western Plus...|  2015-07-17|    Wed|\n",
      "| 2015-08-19|     33|           2|Best Western Plus...|  2015-07-17|    Wed|\n",
      "| 2015-08-13|     27|           2|The Peninsula New...|  2015-07-17|    Thu|\n",
      "| 2015-07-26|      9|           1|Eventi Hotel a Ki...|  2015-07-17|    Sun|\n",
      "| 2015-08-12|     26|           2|Eventi Hotel a Ki...|  2015-07-17|    Wed|\n",
      "| 2015-08-07|     21|           1|Grand Hyatt New York|  2015-07-17|    Fri|\n",
      "| 2015-08-09|     23|           1|Grand Hyatt New York|  2015-07-17|    Sun|\n",
      "| 2015-08-12|     26|           1|Grand Hyatt New York|  2015-07-17|    Wed|\n",
      "| 2015-08-13|     27|           3|Grand Hyatt New York|  2015-07-17|    Thu|\n",
      "| 2015-07-22|      5|           2|Hilton New York F...|  2015-07-17|    Wed|\n",
      "| 2015-07-30|     13|           2|DoubleTree Suites...|  2015-07-17|    Thu|\n",
      "| 2015-07-31|     14|           1|DoubleTree Suites...|  2015-07-17|    Fri|\n",
      "| 2015-07-26|      9|           1|Hampton Inn Manha...|  2015-07-17|    Sun|\n",
      "| 2015-08-02|     16|           1|Hampton Inn Manha...|  2015-07-17|    Sun|\n",
      "| 2015-07-22|      5|           2| Park Hyatt New York|  2015-07-17|    Wed|\n",
      "| 2015-07-22|      5|           3| Park Hyatt New York|  2015-07-17|    Wed|\n",
      "| 2015-07-23|      6|           3| Park Hyatt New York|  2015-07-17|    Thu|\n",
      "| 2015-07-24|      7|           2| Park Hyatt New York|  2015-07-17|    Fri|\n",
      "| 2015-07-24|      7|           3| Park Hyatt New York|  2015-07-17|    Fri|\n",
      "| 2015-07-25|      8|           2| Park Hyatt New York|  2015-07-17|    Sat|\n",
      "+-----------+-------+------------+--------------------+------------+-------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import Row\n",
    "\n",
    "def rowToKeyValue(row):\n",
    "    key = (row['WeekDay'], row[\"Snapshot Date\"], row[\"Checkin Date\"], row[\"DayDiff\"], row[\"Hotel Name\"])\n",
    "    val = ([row[\"Discount Code\"]], row['DiscountPerc'])\n",
    "    return (key,val)\n",
    "\n",
    "def reduceToMaxDiscountPerKey(val1, val2):\n",
    "    codes1, discount1 = val1\n",
    "    codes2, discount2 = val2\n",
    "    if (discount1 > discount2):\n",
    "        return val1\n",
    "    elif(discount2 > discount1):\n",
    "        return val2\n",
    "    else: # In case the discounts are equals, merge the prices to same array\n",
    "        return (codes1+ codes2, discount1)\n",
    "\n",
    "def flatMapDiscountCodes(row):\n",
    "    key, val = row\n",
    "    codes = val[0]\n",
    "    # Return list of key & code\n",
    "    return [(key, code) for code in codes]\n",
    "\n",
    "def rddToRow(rddRow):\n",
    "    return Row(WeekDay=rddRow[0][0], SnapshotDate=rddRow[0][1], CheckinDate=rddRow[0][2],\\\n",
    "                DayDiff=rddRow[0][3], HotelName=rddRow[0][4], DiscountCode=rddRow[1])\n",
    "\n",
    "hotelsBestDiscountCode_df = hotels_df.rdd\\\n",
    "                .map(rowToKeyValue)\\\n",
    "                .reduceByKey(reduceToMaxDiscountPerKey)\\\n",
    "                .flatMap(flatMapDiscountCodes)\\\n",
    "                .map(rddToRow).toDF()\n",
    "hotelsBestDiscountCode_df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Normalize the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+------------+------------+--------------+----------------+-----------------+---------------+---------------+----------------+--------------+\n",
      "|DayDiff|DiscountCode|WeekDayIndex|HotelNameIndex|SnapshotDateYear|SnapshotDateMonth|SnapshotDateDay|CheckinDateYear|CheckinDateMonth|CheckinDateDay|\n",
      "+-------+------------+------------+--------------+----------------+-----------------+---------------+---------------+----------------+--------------+\n",
      "|     26|           2|         0.0|         144.0|            2015|                7|             17|           2015|               8|            12|\n",
      "|     33|           2|         0.0|         144.0|            2015|                7|             17|           2015|               8|            19|\n",
      "|     27|           2|         1.0|          73.0|            2015|                7|             17|           2015|               8|            13|\n",
      "|      9|           1|         6.0|          83.0|            2015|                7|             17|           2015|               7|            26|\n",
      "|     26|           2|         0.0|          83.0|            2015|                7|             17|           2015|               8|            12|\n",
      "|     21|           1|         2.0|          17.0|            2015|                7|             17|           2015|               8|             7|\n",
      "|     23|           1|         6.0|          17.0|            2015|                7|             17|           2015|               8|             9|\n",
      "|     26|           1|         0.0|          17.0|            2015|                7|             17|           2015|               8|            12|\n",
      "|     27|           3|         1.0|          17.0|            2015|                7|             17|           2015|               8|            13|\n",
      "|      5|           2|         0.0|          61.0|            2015|                7|             17|           2015|               7|            22|\n",
      "|     13|           2|         1.0|          77.0|            2015|                7|             17|           2015|               7|            30|\n",
      "|     14|           1|         2.0|          77.0|            2015|                7|             17|           2015|               7|            31|\n",
      "|      9|           1|         6.0|          47.0|            2015|                7|             17|           2015|               7|            26|\n",
      "|     16|           1|         6.0|          47.0|            2015|                7|             17|           2015|               8|             2|\n",
      "|      5|           2|         0.0|          22.0|            2015|                7|             17|           2015|               7|            22|\n",
      "|      5|           3|         0.0|          22.0|            2015|                7|             17|           2015|               7|            22|\n",
      "|      6|           3|         1.0|          22.0|            2015|                7|             17|           2015|               7|            23|\n",
      "|      7|           2|         2.0|          22.0|            2015|                7|             17|           2015|               7|            24|\n",
      "|      7|           3|         2.0|          22.0|            2015|                7|             17|           2015|               7|            24|\n",
      "|      8|           2|         4.0|          22.0|            2015|                7|             17|           2015|               7|            25|\n",
      "+-------+------------+------------+--------------+----------------+-----------------+---------------+---------------+----------------+--------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import col, udf\n",
    "from pyspark.sql.types import IntegerType\n",
    "import pandas as pd\n",
    "\n",
    "# Transform string values to numeric\n",
    "indexers = [StringIndexer(inputCol=\"WeekDay\", outputCol=\"WeekDayIndex\"),\n",
    "            StringIndexer(inputCol=\"HotelName\", outputCol=\"HotelNameIndex\"),]\n",
    "pipeline = Pipeline(stages=indexers)\n",
    "hotelsWithIndexedStrings_df = pipeline.fit(hotelsBestDiscountCode_df).transform(hotelsBestDiscountCode_df)\n",
    "\n",
    "# Extract date values\n",
    "dateYearValue = udf(lambda x: pd.to_datetime(x).year, IntegerType())\n",
    "dateDayValue = udf(lambda x: pd.to_datetime(x).day, IntegerType())\n",
    "dateMonthValue = udf(lambda x: pd.to_datetime(x).month, IntegerType())\n",
    "\n",
    "hotelsWithDateIndexed_df = hotelsWithIndexedStrings_df\\\n",
    "                     .withColumn('SnapshotDateYear', dateYearValue(col('SnapshotDate')))\\\n",
    "                     .withColumn('SnapshotDateMonth', dateMonthValue(col('SnapshotDate')))\\\n",
    "                     .withColumn('SnapshotDateDay', dateDayValue(col('SnapshotDate')))\\\n",
    "                     .withColumn('CheckinDateYear', dateYearValue(col('CheckinDate')))\\\n",
    "                     .withColumn('CheckinDateMonth', dateMonthValue(col('CheckinDate')))\\\n",
    "                     .withColumn('CheckinDateDay', dateDayValue(col('CheckinDate')))\n",
    "# Convert string column to int\n",
    "hotelsWithIntCoulmn_df = hotelsWithDateIndexed_df.withColumn(\"DayDiff\",\\\n",
    "                                   hotelsWithDateIndexed_df[\"DayDiff\"].cast(\"integer\"))\\\n",
    "                                .withColumn('DiscountCode', hotelsWithDateIndexed_df['DiscountCode'].cast('integer'))\n",
    "\n",
    "# Remove unneccesary columns\n",
    "hotelsWithoutColumns_df = hotelsWithIntCoulmn_df.drop('SnapshotDate').drop('CheckinDate')\\\n",
    "                         .drop('HotelName').drop('WeekDay')\n",
    "hotelsWithoutColumns_df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Distinct values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+---+\n",
      "|            index|  0|\n",
      "+-----------------+---+\n",
      "|          DayDiff| 34|\n",
      "|     DiscountCode|  4|\n",
      "|     WeekDayIndex|  7|\n",
      "|   HotelNameIndex|554|\n",
      "| SnapshotDateYear|  2|\n",
      "|SnapshotDateMonth|  7|\n",
      "|  SnapshotDateDay| 31|\n",
      "|  CheckinDateYear|  2|\n",
      "| CheckinDateMonth|  8|\n",
      "|   CheckinDateDay| 31|\n",
      "+-----------------+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import col, countDistinct\n",
    "\n",
    "def transposeDF(df):\n",
    "    pandas_df = df.toPandas().transpose().reset_index()\n",
    "    return spark.createDataFrame(pandas_df)\n",
    "    \n",
    "# Show distinct values count\n",
    "distinctValuesDF = hotelsWithoutColumns_df.agg(*(countDistinct(col(c)).alias(c) for c in hotelsWithoutColumns_df.columns));\n",
    "transposeDF(distinctValuesDF).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Columns statsitics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+------+------------------+------------------+----+-----+\n",
      "|            index|     0|                 1|                 2|   3|    4|\n",
      "+-----------------+------+------------------+------------------+----+-----+\n",
      "|          summary| count|              mean|            stddev| min|  max|\n",
      "|          DayDiff|115580|17.473083578473783|10.042081429292136|   1|   34|\n",
      "|     DiscountCode|115580| 2.410892888042914|1.0198765486288277|   1|    4|\n",
      "|     WeekDayIndex|115580| 2.696954490396262| 1.998808961886477| 0.0|  6.0|\n",
      "|   HotelNameIndex|115580| 65.97189825229279| 80.55926349229914| 0.0|553.0|\n",
      "| SnapshotDateYear|115580|2015.0051046893927|0.0712648263844711|2015| 2016|\n",
      "|SnapshotDateMonth|115580| 9.564544038761031|1.6544450517640836|   1|   12|\n",
      "|  SnapshotDateDay|115580| 16.81938051566015| 8.636772522445504|   1|   31|\n",
      "|  CheckinDateYear|115580|2015.0822028032533|0.2746746350709511|2015| 2016|\n",
      "| CheckinDateMonth|115580|  9.22885447309223|2.7938784591523005|   1|   12|\n",
      "|   CheckinDateDay|115580|16.211541789236893| 8.911702383047402|   1|   31|\n",
      "+-----------------+------+------------------+------------------+----+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Data exploration\n",
    "describe_df = hotelsWithoutColumns_df.describe(hotelsWithoutColumns_df.columns);\n",
    "transposeDF(describe_df).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Column correlations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Column DayDiff correlation: 0.03384978595429131\n",
      "Column DiscountCode correlation: 1.0\n",
      "Column WeekDayIndex correlation: -0.0444292123126588\n",
      "Column HotelNameIndex correlation: -0.05653999461890728\n",
      "Column SnapshotDateYear correlation: 0.0012586486536448303\n",
      "Column SnapshotDateMonth correlation: 0.010092504674174543\n",
      "Column SnapshotDateDay correlation: -0.0065184066646494674\n",
      "Column CheckinDateYear correlation: 0.0017019981329915415\n",
      "Column CheckinDateMonth correlation: 0.008843572569188144\n",
      "Column CheckinDateDay correlation: -0.010942946419650216\n"
     ]
    }
   ],
   "source": [
    "for col in hotelsWithoutColumns_df.columns:\n",
    "    corr = hotelsWithoutColumns_df.corr('DiscountCode', col)\n",
    "    print(\"Column %s correlation: %s\" % (col, corr))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run decision tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.classification import DecisionTreeClassifier\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "\n",
    "# Create vector of all features expect the label\n",
    "assembler = VectorAssembler(\n",
    "    inputCols=[x for x in hotelsWithoutColumns_df.columns if x != 'DiscountCode'],\n",
    "    outputCol='features')\n",
    "\n",
    "# Split the data into training and test sets (30% held out for testing)\n",
    "(trainingData, testData) = hotelsWithoutColumns_df.randomSplit([0.7, 0.3])\n",
    "\n",
    "# Train a DecisionTree model.\n",
    "dt = DecisionTreeClassifier(labelCol='DiscountCode', featuresCol='features',\\\n",
    "                            impurity='entropy', maxDepth=20, maxBins=554)\n",
    "\n",
    "# Chain indexers and tree in a Pipeline\n",
    "pipeline = Pipeline(stages=[assembler, dt])\n",
    "\n",
    "# Train model.  This also runs the indexers.\n",
    "tree_model = pipeline.fit(trainingData)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "labelCol: label column name. (default: label, current: DiscountCode)\n",
      "metricName: metric name in evaluation (f1|weightedPrecision|weightedRecall|accuracy) (default: f1, current: accuracy)\n",
      "predictionCol: prediction column name. (default: prediction, current: prediction)\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "\n",
    "# Make predictions.\n",
    "predictions = tree_model.transform(testData)\n",
    "\n",
    "# Select example rows to display.\n",
    "predictions.select(\"prediction\", \"DiscountCode\", \"features\").show(5)\n",
    "\n",
    "# Select (prediction, true label) and compute test error\n",
    "evaluator = MulticlassClassificationEvaluator(\n",
    "    labelCol=\"DiscountCode\", predictionCol=\"prediction\", metricName=\"accuracy\")\n",
    "accuracy = evaluator.evaluate(predictions)\n",
    "print(\"Test Accuaracy = %g \" %  accuracy)\n",
    "\n",
    "# summary only\n",
    "print(tree_model.stages[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Print decision tree roc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy 0.35315931696659575\n",
      "Class 0 false positive = 0.19500480307396734\n",
      "Class 1 false positive = 0.6282779812265858\n",
      "Class 2 false positive = 0.07761218013096051\n",
      "Class 3 false positive = 0.012743523496962502\n",
      "DenseMatrix([[2552., 4528.,  513.,  127.],\n",
      "             [2058., 8213.,  671.,   87.],\n",
      "             [1727., 6729., 1286.,  151.],\n",
      "             [1493., 3669.,  748.,  234.]])\n",
      "Class 0 area under roc = 0.16528497409326426\n",
      "Class 1 area under roc = 0.37233656723184333\n",
      "Class 2 area under roc = 0.06499545132922269\n",
      "Class 3 area under roc = 0.01904296875\n"
     ]
    }
   ],
   "source": [
    "# from pyspark.mllib.evaluation import MulticlassMetrics, BinaryClassificationMetrics\n",
    "\n",
    "# # MutliClass predictions\n",
    "# predictions = tree_model.predict(test_data.map(lambda p: p.features))\n",
    "# predictionAndLabels = predictions.zip(test_data.map(lambda p: p.label))\n",
    "\n",
    "# metrics = MulticlassMetrics(predictionAndLabels)\n",
    "# print('Accuracy {}'.format(metrics.accuracy))\n",
    "# for label in range(0,4):\n",
    "#     print(\"Class %s false positive = %s\" % (label, metrics.falsePositiveRate(float(label))))\n",
    "    \n",
    "# print(metrics.confusionMatrix())\n",
    "\n",
    "# def mapToBinary(row, label):\n",
    "#     key = row[0]\n",
    "#     val = row[1]\n",
    "#     if (key == label):\n",
    "#         key = 1.0\n",
    "#     else:\n",
    "#         key = 0.0\n",
    "    \n",
    "#     if (val == label):\n",
    "#         val = 1.0\n",
    "#     else:\n",
    "#         val = 0.0\n",
    "#     return (key, val)\n",
    "\n",
    "# # Binary classifications\n",
    "# for label in range(0,4):\n",
    "#     labelPL = predictionAndLabels.filter(lambda row: row[0] == label or row[1] == label)\\\n",
    "#                                  .map(lambda row: mapToBinary(row, label))\n",
    "#     labelMetrics = BinaryClassificationMetrics(labelPL)\n",
    "#     print(\"Class %s area under roc = %s\" % (label, labelMetrics.areaUnderROC))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run naive bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy 0.259734623015873\n",
      "False positive rate 0.2240254536536272\n",
      "DenseMatrix([[  84., 2222.,  208., 4262.],\n",
      "             [ 103., 3363.,  514., 6248.],\n",
      "             [  85., 2804.,  551., 5717.],\n",
      "             [  35., 1479.,  201., 4380.]])\n"
     ]
    }
   ],
   "source": [
    "from pyspark.mllib.classification import NaiveBayes, NaiveBayesModel\n",
    "\n",
    "# Train a naive Bayes model.\n",
    "model = NaiveBayes.train(training_data, 1.0)\n",
    "\n",
    "# Make prediction\n",
    "NaiveBayes_predictionAndLabel = test_data.map(lambda p: (float(model.predict(p.features)), p.label))\n",
    "\n",
    "naive_metrics = MulticlassMetrics(NaiveBayes_predictionAndLabel)\n",
    "\n",
    "print('Accuracy {}'.format(naive_metrics.accuracy))\n",
    "print('False positive rate {}'.format(naive_metrics.weightedFalsePositiveRate))\n",
    "print(naive_metrics.confusionMatrix())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
