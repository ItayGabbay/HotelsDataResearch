{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the csv file and convert features to numeric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Finds the spark path \n",
    "import findspark\n",
    "findspark.init()\n",
    "\n",
    "import pyspark\n",
    "import pyspark.sql\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.ml.feature import StringIndexer\n",
    "from pyspark.ml import Pipeline\n",
    "\n",
    "spark = SparkSession.builder \\\n",
    "     .master(\"local\") \\\n",
    "     .appName(\"hotels\") \\\n",
    "     .getOrCreate()\n",
    "\n",
    "df = spark.read.csv(\"../input/Hotels_data_Changed.csv\", header=True)\n",
    "# Transform string values to numeric\n",
    "indexers = [StringIndexer(inputCol=\"WeekDay\", outputCol=\"WeekDayIndex\"),\n",
    "            StringIndexer(inputCol=\"Hotel Name\", outputCol=\"HotelNameIndex\"),]\n",
    "pipeline = Pipeline(stages=indexers)\n",
    "indexed_df = pipeline.fit(df).transform(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Row(_c0='0', Snapshot ID='1', Snapshot Date='2015-07-17', Checkin Date='2015-08-12', Days='5', Original Price='1178', Discount Price='1040', Discount Code='1', Available Rooms='6', Hotel Name='Best Western Plus Seaport Inn Downtown', Hotel Stars='3', DayDiff='26', WeekDay='Wed', DiscountDiff='138', DiscountPerc='11.714770797962649', WeekDayIndex=0.0, HotelNameIndex=153.0, SnapshotDateIndex=735796, CheckinDateIndex=735822)\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import col, udf\n",
    "from pyspark.sql.types import DoubleType, IntegerType\n",
    "import pandas as pd\n",
    "\n",
    "#Transform date values to numeric\n",
    "dateFormatter = udf(lambda x:  pd.to_datetime(x).toordinal(), IntegerType())\n",
    "\n",
    "indexed_df = indexed_df.withColumn('SnapshotDateIndex', dateFormatter(col('Snapshot Date')))\n",
    "indexed_df = indexed_df.withColumn('CheckinDateIndex', dateFormatter(col('Checkin Date')))\n",
    "\n",
    "print (indexed_df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get the highest discount code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def rowToKeyValue(row):\n",
    "    key = (row['WeekDayIndex'], row[\"SnapshotDateIndex\"], row[\"CheckinDateIndex\"], float(row[\"DayDiff\"]), row[\"HotelNameIndex\"])\n",
    "    val = (row['DiscountPerc'], row[\"Discount Code\"])\n",
    "    return (key,val)\n",
    "\n",
    "def reduceToMaxPrice(val1, val2):\n",
    "    price1, code1 = val1\n",
    "    price2, code2 = val2\n",
    "    if (price1 > price2):\n",
    "        return (price1, code1)\n",
    "    else:\n",
    "        return (price2, code2)\n",
    "    \n",
    "rdd = indexed_df.rdd.map(rowToKeyValue)\\\n",
    "    .reduceByKey(reduceToMaxPrice)\\\n",
    "    .mapValues(lambda val: val[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create test & training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.mllib.tree import DecisionTree, DecisionTreeModel\n",
    "from pyspark.mllib.regression import LabeledPoint\n",
    "\n",
    "def mapToLabeledPoint(tup):\n",
    "    key, val = tup\n",
    "    # Change range of values from 1-4 to 0-3\n",
    "    return LabeledPoint(int(val) -1, list(key))\n",
    "\n",
    "# Split into test and train data\n",
    "test_data, training_data = rdd.map(mapToLabeledPoint).randomSplit(weights=[0.3, 0.7], seed=1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run decision tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "features 0: Week Day\n",
      "features 1: Snapshot Date\n",
      "features 2: Checkin Date\n",
      "features 3: Day Diff\n",
      "features 4: Hotel Name\n"
     ]
    }
   ],
   "source": [
    "tree_model = DecisionTree.trainClassifier(training_data, numClasses=4, \n",
    "                                          categoricalFeaturesInfo={},\n",
    "                                          impurity='gini', maxDepth=20, maxBins=200)\n",
    "\n",
    "# Print results\n",
    "print('features 0: Week Day')\n",
    "print('features 1: Snapshot Date')\n",
    "print('features 2: Checkin Date')\n",
    "print('features 3: Day Diff')\n",
    "print('features 4: Hotel Name')\n",
    "# print(tree_model.toDebugString())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Print decision tree statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy 0.677765376984127\n",
      "False positive rate 0.11711902113715371\n",
      "DenseMatrix([[ 4487.,  1076.,   824.,   480.],\n",
      "             [  938.,  7263.,  1361.,   522.],\n",
      "             [  748.,  1445.,  6352.,   697.],\n",
      "             [  513.,   881.,   909.,  3760.]])\n"
     ]
    }
   ],
   "source": [
    "from pyspark.mllib.evaluation import MulticlassMetrics\n",
    "\n",
    "predictions = tree_model.predict(test_data.map(lambda p: p.features))\n",
    "predictionAndLabels = predictions.zip(test_data.map(lambda p: p.label))\n",
    "\n",
    "metrics = MulticlassMetrics(predictionAndLabels)\n",
    "\n",
    "print('Accuracy {}'.format(metrics.accuracy))\n",
    "print('False positive rate {}'.format(metrics.weightedFalsePositiveRate))\n",
    "print(metrics.confusionMatrix())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run naive bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy 0.25806051587301587\n",
      "False positive rate 0.22261778824380216\n",
      "DenseMatrix([[   82.,  2216.,   230.,  4339.],\n",
      "             [  116.,  3312.,   566.,  6090.],\n",
      "             [   88.,  2847.,   597.,  5710.],\n",
      "             [   42.,  1470.,   218.,  4333.]])\n"
     ]
    }
   ],
   "source": [
    "from pyspark.mllib.classification import NaiveBayes, NaiveBayesModel\n",
    "\n",
    "# Train a naive Bayes model.\n",
    "training_data.toDF().toPandas().to_csv('training.csv')\n",
    "model = NaiveBayes.train(training_data, 1.0)\n",
    "\n",
    "# Make prediction\n",
    "NaiveBayes_predictionAndLabel = test_data.map(lambda p: (float(model.predict(p.features)), p.label))\n",
    "\n",
    "naive_metrics = MulticlassMetrics(NaiveBayes_predictionAndLabel)\n",
    "\n",
    "print('Accuracy {}'.format(naive_metrics.accuracy))\n",
    "print('False positive rate {}'.format(naive_metrics.weightedFalsePositiveRate))\n",
    "print(naive_metrics.confusionMatrix())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
